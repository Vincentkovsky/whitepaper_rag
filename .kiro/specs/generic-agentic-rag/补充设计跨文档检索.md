这是一个非常敏锐的问题。

按照目前的架构（**单文档独立索引**，即一个 document 对应一个 `.pkl` 文件），如果你的需求从\*\*“单文档问答”**变成了**“全库跨文档搜索”**（例如：“在所有白皮书中查找关于‘分片技术’的内容”），目前的实现确实会遇到**性能瓶颈\*\*。

### 1\. 现状：为什么现在“跨文档”很难？

目前的流程是：

1.  用户指定 `document_id`。
2.  `BM25IndexStore` 去磁盘加载**这一个** `.pkl` 文件。
3.  搜索。

如果要跨文档（Global Search），你就不得不：

1.  列出磁盘上所有的 `.pkl` 文件（假设有 1000 个）。
2.  **循环**加载这 1000 个文件到内存。
3.  分别搜索，然后把结果汇总排序。

**后果**：磁盘 IO 爆炸，延迟极高，内存忽高忽低。

-----

### 2\. 解决方案 A：暴力轮询 (适合 MVP / 文档少于 50 份)

如果你现在的文档数量很少（比如只是几十份白皮书），你不需要重构架构，直接在 `HybridRetriever` 里改一点逻辑即可。

**逻辑**：
利用 Python 的多线程（`ThreadPoolExecutor`）并发加载和搜索。

**修改 `HybridRetriever.search` 的伪代码：**

```python
def search(self, query, document_id=None, ...):
    # 场景 A: 指定了文档 ID (现有逻辑)
    if document_id:
        return self._search_single_doc(query, document_id)
    
    # 场景 B: 跨文档搜索 (新增逻辑)
    else:
        # 1. 获取所有文档 ID
        all_doc_ids = self._bm25_store.list_indexes()
        
        # 2. 并行搜索 (BM25读取文件是IO密集型，适合多线程)
        all_bm25_results = []
        with ThreadPoolExecutor(max_workers=10) as executor:
            # 提交所有搜索任务
            futures = [
                executor.submit(self._search_single_bm25, query, doc_id) 
                for doc_id in all_doc_ids
            ]
            
            # 收集结果
            for f in as_completed(futures):
                all_bm25_results.extend(f.result())
        
        # 3. 向量检索也改成不带 document_id 过滤
        vector_results = self._vector_search_global(query)
        
        # 4. RRF 融合 (算法不变)
        return self._rrf_fusion(vector_results, all_bm25_results)
```

**优点**：代码改动小，立刻能用。
**缺点**：文档多了（几百份）之后会显著变慢。

-----

### 3\. 解决方案 B：全局索引 (Global Index) - (中等规模推荐)

如果你确定你的主要场景是“跨文档搜索”，那么你应该改变存储策略：**不再按文档存，而是存一个大的。**

**策略**：

1.  创建一个名为 `GLOBAL_INDEX` 的特殊 ID。
2.  每当用户上传新文档，不创建新文件，而是把新文档的 chunks **追加** 到 `GLOBAL_INDEX` 对应的 `BM25Service` 中，然后重新 `build_index` 并保存。
3.  搜索时，直接加载 `GLOBAL_INDEX.pkl`。

**优点**：搜索速度极快（一次内存查找）。
**缺点**：

  * **写入慢**：每传一个新文档，都要重算整个库的 TF-IDF（全量构建）。
  * **内存压力**：这个全局对象的内存占用会随着文档增加而线性增长。

-----

### 4\. 解决方案 C：更换基础设施 (大规模推荐)

如果你的目标是生产级、海量文档的跨文档检索，那么**本地文件方案（Pickle）就走到头了**。

你需要引入支持**稀疏向量（Sparse Vector）或全文检索**的数据库。

**选项 1：Qdrant / Milvus (推荐)**

  * 现在的向量数据库（如 Qdrant）支持 **Hybrid Search**（Dense Vector + Sparse Vector）。
  * **Sparse Vector** 本质上就是 BM25 的变体。
  * **迁移**：你把 BM25 的计算逻辑换成生成 Sparse Vector，然后和 Dense Vector 一起存进 Qdrant。对账、存储、检索全部由数据库接管。

**选项 2：Elasticsearch**

  * 把文本存进 ES。ES 天生就是做倒排索引（BM25）的。
  * `HybridRetriever` 变成：调 Chroma 拿向量结果，调 ES 拿关键词结果，然后 RRF 融合。

-----

### 建议

**鉴于你目前的阶段（Agentic RAG MVP）：**

1.  **保持现状**。Agentic RAG 的特点通常是 **“先定位，后分析”**。

      * Router 现在的设计逻辑是：用户问问题 -\> Router 决定调用 Document Search。
      * 你可以增加一步：**Router 决定调用 `List Documents` 工具**，让 LLM 先选出“看似相关”的 3-5 个文档，然后再针对这 3-5 个文档 ID 进行检索。
      * **这避免了全库扫描，符合 Agent 的行为模式。**

2.  如果非要支持“全库盲搜”：

      * 采用 **解决方案 A (多线程暴力轮询)**。
      * 在 `AgentConfig` 里加个限制，比如 `MAX_CROSS_DOC_SEARCH = 20`，最多只允许跨 20 个文档搜，再多就报错提示用户缩小范围。这能保护你的服务器。